{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "뉴스 데이터셋을 활용하여 **추상적 요약**과 **추출적 요약**을 진행하겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  사용할 모듈 import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import nltk\n",
    "#nltk.download('stopwords')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#from nltk.corpus import stopwords\n",
    "from bs4 import BeautifulSoup \n",
    "from tensorflow.keras.preprocessing.text import Tokenizer \n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "import requests   # 추출적 요약\n",
    "from summa.summarizer import summarize # 추출적 요약\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 수집하기\n",
    "사용할 뉴스 기사 데이터 셋 : **[sunnysai12345/News_Summary](https://github.com/sunnysai12345/News_Summary)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래의 코드로 데이터 셋 다운로드 할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/sunnysai12345/News_Summary/master/news_summary_more.csv\", filename=\"news_summary_more.csv\")\n",
    "data = pd.read_csv('news_summary_more.csv', encoding='iso-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headlines</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>78676</th>\n",
       "      <td>Nitish-Lalu alliance was a mismatch: Union Law...</td>\n",
       "      <td>Union Law Minister Ravi Shankar Prasad on Wedn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33552</th>\n",
       "      <td>Ex-Britannia exec's dairy firm Happy Cow raise...</td>\n",
       "      <td>Mumbai-based dairy startup Happy Cow Dairy has...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70717</th>\n",
       "      <td>McGregor sued for Ã¢ÂÂ¹60 lakh over bottle-th...</td>\n",
       "      <td>UFC champion Conor McGregor has been sued by a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54766</th>\n",
       "      <td>Pak's spy agency trying to revive Khalistan mo...</td>\n",
       "      <td>Operatives of Pakistan's espionage agency ISI ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73356</th>\n",
       "      <td>Reliance Jio temporarily suspends JioPhone pre...</td>\n",
       "      <td>Reliance Jio has temporarily suspended pre-boo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20055</th>\n",
       "      <td>Got army of brothers, Siddharth's leader by de...</td>\n",
       "      <td>Actress Priyanka Chopra shared a picture with ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69337</th>\n",
       "      <td>Star found to spin at 320 km/sec, near to 'bre...</td>\n",
       "      <td>Australia-based scientists have found a star n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98396</th>\n",
       "      <td>CRPF jawan axed to death by Maoists in Chhatti...</td>\n",
       "      <td>A CRPF jawan was on Tuesday axed to death with...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9570</th>\n",
       "      <td>Apple to delete WhatsApp sticker apps from App...</td>\n",
       "      <td>Apple is planning to delete all WhatsApp stick...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79114</th>\n",
       "      <td>18-month-old Mumbai girl dies after being hit ...</td>\n",
       "      <td>An 18-month-old girl reportedly died on Saturd...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               headlines  \\\n",
       "78676  Nitish-Lalu alliance was a mismatch: Union Law...   \n",
       "33552  Ex-Britannia exec's dairy firm Happy Cow raise...   \n",
       "70717  McGregor sued for Ã¢ÂÂ¹60 lakh over bottle-th...   \n",
       "54766  Pak's spy agency trying to revive Khalistan mo...   \n",
       "73356  Reliance Jio temporarily suspends JioPhone pre...   \n",
       "20055  Got army of brothers, Siddharth's leader by de...   \n",
       "69337  Star found to spin at 320 km/sec, near to 'bre...   \n",
       "98396  CRPF jawan axed to death by Maoists in Chhatti...   \n",
       "9570   Apple to delete WhatsApp sticker apps from App...   \n",
       "79114  18-month-old Mumbai girl dies after being hit ...   \n",
       "\n",
       "                                                    text  \n",
       "78676  Union Law Minister Ravi Shankar Prasad on Wedn...  \n",
       "33552  Mumbai-based dairy startup Happy Cow Dairy has...  \n",
       "70717  UFC champion Conor McGregor has been sued by a...  \n",
       "54766  Operatives of Pakistan's espionage agency ISI ...  \n",
       "73356  Reliance Jio has temporarily suspended pre-boo...  \n",
       "20055  Actress Priyanka Chopra shared a picture with ...  \n",
       "69337  Australia-based scientists have found a star n...  \n",
       "98396  A CRPF jawan was on Tuesday axed to death with...  \n",
       "9570   Apple is planning to delete all WhatsApp stick...  \n",
       "79114  An 18-month-old girl reportedly died on Saturd...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 데이터는 기사의 본문에 해당되는 text와 headlines 두 가지 열로 구성되어져 있습니다.\n",
    "\n",
    "**추상적 요약**을 하는 경우에는 text를 본문, headlines를 이미 요약된 데이터로 삼아서 모델을 학습할 수 있어요. \n",
    "\n",
    "그리고 **추출적 요약**을 하는 경우에는 text열만 사용하면됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 전처리하기 (추상적 요약)\n",
    "- 중복 및 NULL값 제거\n",
    "- 텍스트 정규화 및 정제\n",
    "- 불용어 제거\n",
    "- 훈련데이터와 테스트데이터 나누기\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 중복 샘플과 NULL 값이 존재하는 샘플 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플수 : 98401\n"
     ]
    }
   ],
   "source": [
    "print('전체 샘플수 :',(len(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Headlines 열에서 중복을 배제한 유일한 샘플의 수 : 98280\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# text_max_len = 40\n",
    "# -> 0.9 이상print('Text 열에서 중복을 배제한 유일한 샘플의 수 :', data['text'].nunique())\n",
    "print('Headlines 열에서 중복을 배제한 유일한 샘플의 수 :', data['headlines'].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "text열의 경우 중복을 제외한다면  98,360개  , Headlines에는 98,280개의 유일한 데이터가 존재합니다. 여기서 Headlines의 경우 원문 (text)에서의 내용이 달라도 핵심 내용이 같을 순 있기에  Headlines의 경우는 중복이 있을 수 있습니다.\n",
    "\n",
    "그렇지만, text의 경우 중복이 존재한다면 아예 같은 내용을 말하는 것이기에 text열의 중복 데이터는 제거해야 해요.\n",
    "\n",
    "데이터프레임의 **`drop_duplicates()`**를 사용하면, 손쉽게 중복 샘플을 제거할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플수 : 98360\n"
     ]
    }
   ],
   "source": [
    "data.drop_duplicates(subset = ['text'], inplace = True)\n",
    "print('전체 샘플수 :',(len(data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "중복이 제거되면서 샘플 수가 98,360개로 줄어들었습니다. \n",
    "\n",
    "그런데 만약 데이터가 **Null**값을 가지는 샘플이 있었다면, **`drop_duplicates()`**가 중복된 Null들을 지워주기는 하겠지만, 여전히 Null값 한개가 어딘가 남아있을 수 있어요. \n",
    "\n",
    "데이터에 Null 값이 남아있는지 확인해 봅시다.~\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "headlines    0\n",
      "text         0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(data.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 0으로 나오는 것을 보니 null 값이 아예 없었던것 같네요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플수 : 98360\n"
     ]
    }
   ],
   "source": [
    "print('전체 샘플수 :',(len(data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이렇게 중복, NULL값의 처리를 통해 데이터의 샘플이 총 98,360개가 되었습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 텍스트 정규화와 불용어 제거\n",
    "살아남은 98,360개의 샘플에는 수많은 단어들이 있습니다. 그런데 사실 그 단어들 중에서는 같은 의미인데도 다른 표현으로 쓰여 마치 다른 단어들처럼 간주되는 경우가 있어요.\n",
    "\n",
    "예를 들어서 'i'll'은 'i will'과 같고, 'mightn't과 'might not'은 사실 같은 표현이죠. 이런 경우 기계가 굳이 이들을 마치 다른 단어로 간주하게 해서 연산량을 늘리는 것보다는 기계 학습 전에 미리 같은 표현으로 통일시켜주는 것이 기계의 연산량을 줄일 수 있는 방법입니다.\n",
    "\n",
    "이러한 방법론을 텍스트 처리에서는 **`텍스트 정규화(text normalization)`**라고 해요.\n",
    "\n",
    "여기서는 텍스트 정규화를 위한 사전(dictionary)을 아래와 같이 구성하겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 텍스트 정규화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정규화 사전의 수:  120\n"
     ]
    }
   ],
   "source": [
    "contractions = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
    "                           \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
    "                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
    "                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n",
    "                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
    "                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
    "                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
    "                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n",
    "                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
    "                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
    "                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n",
    "                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
    "                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
    "                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n",
    "                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n",
    "                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
    "                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
    "                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
    "                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
    "                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n",
    "                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
    "                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
    "                           \"you're\": \"you are\", \"you've\": \"you have\"}\n",
    "\n",
    "print(\"정규화 사전의 수: \",len(contractions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 불용어(stopwords)\n",
    "그리고 일반적으로 텍스트에는 자주 등장하지만 자연어 처리를 할 때 실질적으로 별 도움이 되지 않는 단어들이 존재해요. 이를 라고 **`불용어(stopwords)`**라고 부릅니다.\n",
    "\n",
    "때로는 불용어를 제거하는 것이 자연어 처리의 성능을 높이는 방법일 수 있어요. 여기서는 **NLTK**에서 제공하는 불용어 리스트를 참조해, 샘플에서 불용어를 제거하겠습니다.\n",
    "\n",
    "- **NLTK**는 `Natural Language Toolkit`의 축약어로 영어 기호, 통계, 자연어 처리를 위한 라이브러리로 문장에는 자주 등장하지만, 의미를 분석하고 요약하는데는 거의 의미가 없는 100여개의 불용어가 미리 정리되어있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ssac27/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "불용어 개수 : 179\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print('불용어 개수 :', len(stopwords.words('english') ))\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#데이터 전처리 함수\n",
    "def preprocess_sentence(sentence, remove_stopwords=True):\n",
    "    sentence = sentence.lower() # 텍스트 소문자화\n",
    "    sentence = BeautifulSoup(sentence, \"lxml\").text # <br />, <a href = ...> 등의 html 태그 제거\n",
    "    sentence = re.sub(r'\\([^)]*\\)', '', sentence) # 괄호로 닫힌 문자열 (...) 제거 Ex) my husband (and myself!) for => my husband for\n",
    "    sentence = re.sub('\"','', sentence) # 쌍따옴표 \" 제거\n",
    "    sentence = ' '.join([contractions[t] if t in contractions else t for t in sentence.split(\" \")]) # 약어 정규화\n",
    "    sentence = re.sub(r\"'s\\b\",\"\",sentence) # 소유격 제거. Ex) roland's -> roland\n",
    "    sentence = re.sub(\"[^a-zA-Z]\", \" \", sentence) # 영어 외 문자(숫자, 특수문자 등) 공백으로 변환\n",
    "    sentence = re.sub('[m]{2,}', 'mm', sentence) # m이 3개 이상이면 2개로 변경. Ex) ummmmmmm yeah -> umm yeah\n",
    "    \n",
    "    # 불용어 제거 (Text)\n",
    "    if remove_stopwords:\n",
    "        tokens = ' '.join(word for word in sentence.split() if not word in stopwords.words('english') if len(word) > 1)\n",
    "    # 불용어 미제거 (Summary)\n",
    "    else:\n",
    "        tokens = ' '.join(word for word in sentence.split() if len(word) > 1)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터 전처리기 제대로 되었는지 확인해 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "불용어 제거 o\n",
      "everything bought great infact ordered twice third ordered wasfor mother father\n",
      "불용어 제거 x\n",
      "great way to start the day\n"
     ]
    }
   ],
   "source": [
    "temp_text = 'Everything I bought was great, infact I ordered twice and the third ordered was<br />for my mother and father.'\n",
    "temp_summary = 'Great way to start (or finish) the day!!!'\n",
    "\n",
    "print(\"불용어 제거 o\")\n",
    "print(preprocess_sentence(temp_text))\n",
    "print(\"불용어 제거 x\")\n",
    "print(preprocess_sentence(temp_summary, False))  # 불용어를 제거하지 않습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "결과를 보면 기본적으로 모든 알파벳이 소문자로 변환되고, 과 같은 html 태그가 제거되었습니다. 또한 (or finish)와 같은 괄호로 묶였던 단어 시퀀스가 제거된 것도 확인할 수 있어요. \n",
    "\n",
    "그리고 특수문자가 제거되면서 영어만 남았습니다.\n",
    "\n",
    "훈련 데이터 전체에 대해서 전처리를 수행해볼게요. 이때, **text의 경우에는 불용어를 제거**하고, **headlines의 경우에는 불용어를 제거**하지 않을 것이므로 따로 호출해서 진행해야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['saurav kant alumnus upgrad iiit pg program machine learning artificial intelligence sr systems engineer infosys almost years work experience program upgrad degree career support helped transition data scientist tech mahindra salary hike upgrad online power learning powered lakh careers',\n",
       " 'kunal shah credit card bill payment platform cred gave users chance win free food swiggy one year pranav kaushik delhi techie bagged reward spending cred coins users get one cred coin per rupee bill paid used avail rewards brands like ixigo bookmyshow ubereats cult fit',\n",
       " 'new zealand defeated india wickets fourth odi hamilton thursday win first match five match odi series india lost international match rohit sharma captaincy consecutive victories dating back march match witnessed india getting seventh lowest total odi cricket history',\n",
       " 'aegon life iterm insurance plan customers enjoy tax benefits premiums paid save taxes plan provides life cover age years also customers options insure critical illnesses disability accidental death benefit rider life cover age years',\n",
       " 'speaking sexual harassment allegations rajkumar hirani sonam kapoor said known hirani many years true metoo movement get derailed metoo movement always believe woman case need reserve judgment added hirani accused assistant worked sanju']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text = []\n",
    "\n",
    "# 전체 Text 데이터에 대한 전처리 \n",
    "# 시간이 오래 걸릴 수 있습니다.\n",
    "for s in data['text']:\n",
    "    clean_text.append(preprocess_sentence(s))\n",
    "\n",
    "# 전처리 후 출력\n",
    "clean_text[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 headlines에 대해서 전처리 함수를 호출해줄 때는, 불용어 제거를 수행하지 않는다는 의미에서 두번째 인자로 False를 넣어주겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['upgrad learner switches to career in ml al with salary hike',\n",
       " 'delhi techie wins free food from swiggy for one year on cred',\n",
       " 'new zealand end rohit sharma led india match winning streak',\n",
       " 'aegon life iterm insurance plan helps customers save tax',\n",
       " 'have known hirani for yrs what if metoo claims are not true sonam']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_summary = []\n",
    "\n",
    "# 전체 Summary 데이터에 대한 전처리 \n",
    "# 시간이 오래 걸릴 수 있습니다.\n",
    "for s in data['headlines']:\n",
    "    clean_summary.append(preprocess_sentence(s, False))\n",
    "\n",
    "clean_summary[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이렇게 텍스트 정제의 과정을 거친 후에는 다시 한번 **빈(empty)** 샘플이 생겼는지 확인해보는 것이 좋습니다.  \n",
    "정제 전에는 데이터가 존재했지만, 정제 과정에서 문장의 모든 단어가 사라지는 경우가 있을 수 있어요. 이렇게 되면 샘플 자체가 빈 값을 가지게 됩니다.\n",
    "\n",
    "이를 보다 쉽게 확인하기 위해 데이터들을 데이터프레임에 다시 저장하고고 빈(empty) 값을 가진 샘플들이 있다면, 모두 Null 값을 가진 샘플로 대체하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text'] = clean_text\n",
    "data['headlines'] = clean_summary\n",
    "\n",
    "# 빈 값을 Null 값으로 변환\n",
    "data.replace('', np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이전과 같이 .isnull().sum()을 사용해서 Null 값이 생겼는지 해볼게요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "headlines    0\n",
       "text         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모든 데이터 샘플들에서 Null값이 생기지 않았습니다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 훈련데이터와 테스트데이터 나누기\n",
    "\n",
    "학습을 진행하기 위해서는 학습에 사용할 데이터의 크기를 결정하고, 문장의 시작과 끝을 표시 해줘야합니다.\n",
    "\n",
    "### 샘플의 최대 길이 정하기\n",
    "\n",
    "필요없는 단어를 모두 솎아낸 데이터를 가지게 되었으니, 이제 훈련에 사용할 샘플의 최대 길이를 정해주겠습니다.\n",
    "\n",
    "text와 headlines의 최소, 최대, 평균 길이를 구하고 또한 길이 분포를 시각화해서 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "텍스트의 최소 길이 : 1\n",
      "텍스트의 최대 길이 : 60\n",
      "텍스트의 평균 길이 : 35.09968483123221\n",
      "요약의 최소 길이 : 1\n",
      "요약의 최대 길이 : 16\n",
      "요약의 평균 길이 : 9.299532330215534\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 길이 분포 출력\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "text_len = [len(s.split()) for s in data['text']]\n",
    "headlines_len = [len(s.split()) for s in data['headlines']]\n",
    "\n",
    "print('텍스트의 최소 길이 : {}'.format(np.min(text_len)))\n",
    "print('텍스트의 최대 길이 : {}'.format(np.max(text_len)))\n",
    "print('텍스트의 평균 길이 : {}'.format(np.mean(text_len)))\n",
    "print('요약의 최소 길이 : {}'.format(np.min(headlines_len)))\n",
    "print('요약의 최대 길이 : {}'.format(np.max(headlines_len)))\n",
    "print('요약의 평균 길이 : {}'.format(np.mean(headlines_len)))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.boxplot(headlines_len)\n",
    "plt.title('headlines')\n",
    "plt.subplot(1,2,2)\n",
    "plt.boxplot(text_len)\n",
    "plt.title('text')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.title('headlines')\n",
    "plt.hist(headlines_len, bins = 40)\n",
    "plt.xlabel('length of samples')\n",
    "plt.ylabel('number of samples')\n",
    "plt.show()\n",
    "\n",
    "plt.title('text')\n",
    "plt.hist(text_len, bins = 40)\n",
    "plt.xlabel('length of samples')\n",
    "plt.ylabel('number of samples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 그래프처럼, 많은 양의 데이터를 다룰때는 데이터를 시각화하여 보는 것이 도움이 돼요. 위에서 부터 차례대로 그래프는 각각 요약과 실제 텍스트의 길이 분포, 요약본 샘플 길이별 갯수, 실제 텍스트 샘플 길이별 갯수를 나타내고 있어요.\n",
    "\n",
    "Text의 경우 최소 길이가 1, 최대 길이가 60으로 그 차이를 보입니다. 그리고 평균 길이는 35로 시각화 된 그래프로 봤을 때는 대체적으로는 분포가 안정적으로 보입니다.\n",
    "\n",
    "\n",
    "headlines의 경우 최소 길이가 1, 최대 길이가 16, 그리고 평균 길이가 9로 Text에 비해 상대적으로 길이가 짧습니다. \n",
    "\n",
    "이로부터 Text의 최대 길이와 headlines의 최대 길이를 임의로 정해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_max_len = 45\n",
    "\n",
    "headlines_max_len = 14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "text의 최대는 45로 headlines의 경우 14로 정했는데 이 길이를 선택했을 때, \n",
    "얼마나 많은 샘플들을 자르지 않고 포함할 수 있는지 통계로 확인하는 편이 객관적으로 길이를 결정하는데 도움이 됩니다. \n",
    "\n",
    "훈련 데이터와 샘플의 길이를 입력하면, 데이터의 몇 %가 해당하는지 계산하는 함수를 만들어서 좀 더 정확하게 판단해볼게요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def below_threshold_len( max_len, nested_list):\n",
    "    cnt = 0\n",
    "    for s in nested_list:\n",
    "        if (len(s.split()) <= max_len) :\n",
    "            cnt = cnt + 1\n",
    "    print('전체 샘플 중 길이가 %s 이하인 샘플의 비율: %s'%(max_len, (cnt / len(nested_list))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이렇게 만든 함수를 text와 headlines에 적용해 우리가 결정한 임의의 길이가 몇%의 샘플까지 포함하는지 볼 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플 중 길이가 45 이하인 샘플의 비율: 0.9967771451809678\n",
      "전체 샘플 중 길이가 14 이하인 샘플의 비율: 0.9997763318422123\n"
     ]
    }
   ],
   "source": [
    "below_threshold_len(text_max_len, data['text'])\n",
    "below_threshold_len(headlines_max_len, data['headlines'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2개 모두 99이상이니 좀더 범위를 좁혀 봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_max_len = 38  # 0.816\n",
    "\n",
    "headlines_max_len = 10  # 0.816\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플 중 길이가 38 이하인 샘플의 비율: 0.8163379422529483\n",
      "전체 샘플 중 길이가 10 이하인 샘플의 비율: 0.8162972753151687\n"
     ]
    }
   ],
   "source": [
    "below_threshold_len( text_max_len, data['text'])\n",
    "below_threshold_len(headlines_max_len, data['headlines'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이정도로 맞추고 데이터 정제를 진행하겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각각 38과 10으로 패딩을 하게되면 해당 길이보다 긴 샘플들은 내용이 잘리게 됩니다. \n",
    "\n",
    "정해진 길이에 맞춰 자르는 것이 아니라, 정해진 길이보다 길면 제외하는 방법으로 데이터를 정제하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플수 : 65295\n"
     ]
    }
   ],
   "source": [
    "data = data[data['text'].apply(lambda x: len(x.split()) <= text_max_len)]\n",
    "data = data[data['headlines'].apply(lambda x: len(x.split()) <= headlines_max_len)]\n",
    "print('전체 샘플수 :',(len(data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 시작 토큰과 종료 토큰 추가하기\n",
    "\n",
    "디코더는 시작 토큰을 입력받아 문장을 생성하기 시작하고, 종료 토큰을 예측한 순간에 문장 생성을 멈추게 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "seq2seq 훈련을 위해서는 디코더의 입력과 레이블에 시작 토큰과 종료 토큰을 추가할 필요가 있습니다.  이 프로젝트에서는 **시작 토큰은 'sostoken'**, **종료 토큰은 'eostoken'**이라 임의로 명명하고 앞, 뒤로 추가하겠습니다. \n",
    "\n",
    "디코더의 입력에 해당하면서 시작 토큰이 맨 앞에 있는 문장의 이름을 **decoder_input**,\n",
    "디코더의 출력 또는 레이블에 해당되면서 종료 토큰이 맨 뒤에 붙는 문장의 이름을 **decoder_target**이라고 이름을 정하겠습니다.\n",
    "\n",
    "두 개의 문장 모두 headlines 열로부터 만들면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headlines</th>\n",
       "      <th>text</th>\n",
       "      <th>decoder_input</th>\n",
       "      <th>decoder_target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>new zealand end rohit sharma led india match w...</td>\n",
       "      <td>new zealand defeated india wickets fourth odi ...</td>\n",
       "      <td>sostoken new zealand end rohit sharma led indi...</td>\n",
       "      <td>new zealand end rohit sharma led india match w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aegon life iterm insurance plan helps customer...</td>\n",
       "      <td>aegon life iterm insurance plan customers enjo...</td>\n",
       "      <td>sostoken aegon life iterm insurance plan helps...</td>\n",
       "      <td>aegon life iterm insurance plan helps customer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>rahat fateh ali khan denies getting notice for...</td>\n",
       "      <td>pakistani singer rahat fateh ali khan denied r...</td>\n",
       "      <td>sostoken rahat fateh ali khan denies getting n...</td>\n",
       "      <td>rahat fateh ali khan denies getting notice for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>cong wins ramgarh bypoll in rajasthan takes to...</td>\n",
       "      <td>congress candidate shafia zubair ramgarh assem...</td>\n",
       "      <td>sostoken cong wins ramgarh bypoll in rajasthan...</td>\n",
       "      <td>cong wins ramgarh bypoll in rajasthan takes to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>up cousins fed human excreta for friendship wi...</td>\n",
       "      <td>two minor cousins uttar pradesh gorakhpur alle...</td>\n",
       "      <td>sostoken up cousins fed human excreta for frie...</td>\n",
       "      <td>up cousins fed human excreta for friendship wi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            headlines  \\\n",
       "2   new zealand end rohit sharma led india match w...   \n",
       "3   aegon life iterm insurance plan helps customer...   \n",
       "5   rahat fateh ali khan denies getting notice for...   \n",
       "9   cong wins ramgarh bypoll in rajasthan takes to...   \n",
       "10  up cousins fed human excreta for friendship wi...   \n",
       "\n",
       "                                                 text  \\\n",
       "2   new zealand defeated india wickets fourth odi ...   \n",
       "3   aegon life iterm insurance plan customers enjo...   \n",
       "5   pakistani singer rahat fateh ali khan denied r...   \n",
       "9   congress candidate shafia zubair ramgarh assem...   \n",
       "10  two minor cousins uttar pradesh gorakhpur alle...   \n",
       "\n",
       "                                        decoder_input  \\\n",
       "2   sostoken new zealand end rohit sharma led indi...   \n",
       "3   sostoken aegon life iterm insurance plan helps...   \n",
       "5   sostoken rahat fateh ali khan denies getting n...   \n",
       "9   sostoken cong wins ramgarh bypoll in rajasthan...   \n",
       "10  sostoken up cousins fed human excreta for frie...   \n",
       "\n",
       "                                       decoder_target  \n",
       "2   new zealand end rohit sharma led india match w...  \n",
       "3   aegon life iterm insurance plan helps customer...  \n",
       "5   rahat fateh ali khan denies getting notice for...  \n",
       "9   cong wins ramgarh bypoll in rajasthan takes to...  \n",
       "10  up cousins fed human excreta for friendship wi...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# headlines 데이터에 시작 토큰과 종료 토큰을 추가한다.\n",
    "data['decoder_input'] = data['headlines'].apply(lambda x : 'sostoken '+ x)\n",
    "data['decoder_target'] = data['headlines'].apply(lambda x : x + ' eostoken')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "인코더의 입력, 디코더의 입력과 레이블을 각각 다시 Numpy 타입으로 저장한 후 데이터를 훈련데이터와 테스트 데이터로 분리하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = np.array(data['text']) # 인코더의 입력\n",
    "decoder_input = np.array(data['decoder_input']) # 디코더의 입력\n",
    "decoder_target = np.array(data['decoder_target']) # 디코더의 레이블"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "직접 코딩을 통해서 데이터를 분리하겠습니다.  \n",
    "우선, encoder_input과 크기와 형태가 같은 순서가 섞인 정수 시퀀스를 만들어줄게요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[60598  4782 10171 ... 60162 48070  2454]\n"
     ]
    }
   ],
   "source": [
    "indices = np.arange(encoder_input.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 정수 시퀀스를 이용해 다시 데이터의 샘플 순서를 정의해주면 잘 섞인 샘플이 되겠죠."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = encoder_input[indices]\n",
    "decoder_input = decoder_input[indices]\n",
    "decoder_target = decoder_target[indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 섞인 데이터를 8:2의 비율로 훈련 데이터와 테스트 데이터로 분리해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테스트 데이터의 수 : 13059\n"
     ]
    }
   ],
   "source": [
    "n_of_val = int(len(encoder_input)*0.2)\n",
    "print('테스트 데이터의 수 :',n_of_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이렇게 정의한 테스트 데이터의 갯수를 이용해 전체 데이터를 양분할게요. :표시의 위치에 주의해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 데이터의 개수 : 52236\n",
      "훈련 레이블의 개수 : 52236\n",
      "테스트 데이터의 개수 : 13059\n",
      "테스트 레이블의 개수 : 13059\n"
     ]
    }
   ],
   "source": [
    "encoder_input_train = encoder_input[:-n_of_val]\n",
    "decoder_input_train = decoder_input[:-n_of_val]\n",
    "decoder_target_train = decoder_target[:-n_of_val]\n",
    "\n",
    "encoder_input_test = encoder_input[-n_of_val:]\n",
    "decoder_input_test = decoder_input[-n_of_val:]\n",
    "decoder_target_test = decoder_target[-n_of_val:]\n",
    "\n",
    "print('훈련 데이터의 개수 :', len(encoder_input_train))\n",
    "print('훈련 레이블의 개수 :',len(decoder_input_train))\n",
    "print('테스트 데이터의 개수 :',len(encoder_input_test))\n",
    "print('테스트 레이블의 개수 :',len(decoder_input_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 정수 인코딩\n",
    "\n",
    "\n",
    "### 단어 집합(vocaburary) 만들기 및 정수 인코딩\n",
    "\n",
    "이제 기계가 텍스트를 숫자로 처리할 수 있도록 훈련 데이터와 테스트 데이터의 단어들을 모두 \n",
    "각 단어에 고유한 정수로 매핑하는 작업이 필요합니다.\n",
    "\n",
    "- 우선, 원문에 해당되는 **`encoder_input_train`**에 대해서 단어 집합을 만들겠습니다.\n",
    "\n",
    "Keras의 토크나이저를 사용하면, 입력된 훈련 데이터로부터 단어 집합을 만들 수 있어요.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_tokenizer = Tokenizer() # 토크나이저 정의\n",
    "src_tokenizer.fit_on_texts(encoder_input_train) # 입력된 데이터로부터 단어 집합 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 단어 집합이 생성되는 동시에 각 단어에 고유한 정수가 부여되었어요. 현재 생성된 단어 집합은 **src_tokenizer.word_index**에 저장되어 있습니다. 이때, 모든 단어를 사용하는 것이 아니라, 빈도수가 낮은 단어들은 훈련 데이터에서 제외하고 진행합니다.\n",
    "\n",
    "등장 빈도수가 낮은 단어들을 확인해 보겠습니다. \n",
    "\n",
    "- **src_tokenizer.word_counts.items()**에는 단어와 각 단어의 등장 빈도수가 저장되어져 있는데, 이를 통해서 통계적인 정보를 얻을 수 있어요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합(vocabulary)의 크기 : 57271\n",
      "등장 빈도가 7번 이하인 희귀 단어의 수: 40600\n",
      "단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기 16671\n",
      "단어 집합에서 희귀 단어의 비율: 70.89102687223901\n",
      "전체 등장 빈도에서 희귀 단어 등장 빈도 비율: 5.015675750418349\n"
     ]
    }
   ],
   "source": [
    "threshold = 8  # 등장 빈도수 지정\n",
    "total_cnt = len(src_tokenizer.word_index) # 단어의 수\n",
    "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
    "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
    "rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
    "\n",
    "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
    "for key, value in src_tokenizer.word_counts.items():\n",
    "    total_freq = total_freq + value\n",
    "\n",
    "    # 단어의 등장 빈도수가 threshold보다 작으면\n",
    "    if(value < threshold):\n",
    "        rare_cnt = rare_cnt + 1\n",
    "        rare_freq = rare_freq + value\n",
    "\n",
    "print('단어 집합(vocabulary)의 크기 :',total_cnt)\n",
    "print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n",
    "print('단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기 %s'%(total_cnt - rare_cnt))\n",
    "print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n",
    "print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "encoder_input_train에는 총 57,271개의 단어가 있네요. 그 아래의 통계 정보들을 해석해 보면,\n",
    "\n",
    "등장 빈도가 threshold 값인 8회 미만, 즉, 7회 이하인 단어들은 단어 집합에서 70.8% 이상을 차지하네요. 하지만, 실제로 훈련 데이터에서 등장 빈도로 차지하는 비중은 상대적으로 적은 수치인 5.01%밖에 되지 않아요.\n",
    "\n",
    "그래서 등장 빈도가 7회 이하인 단어들은 정수 인코딩 과정에서 빼고, 훈련 데이터에서 제거하고자 합니다.\n",
    "위에서 이를 제외한 단어 집합의 크기를 16,671으로 계산했는데, 이와 비슷한 값으로 어림잡아 단어 집합의 크기를 16,500으로 제한하겠습니다. \n",
    "- 토크나이저를 정의할 때 num_words의 값을 정해주면, 단어 집합의 크기를 제한할 수 있어요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_vocab = 16500\n",
    "src_tokenizer = Tokenizer(num_words = src_vocab) # 단어 집합의 크기를 8,000으로 제한\n",
    "src_tokenizer.fit_on_texts(encoder_input_train) # 단어 집합 재생성."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**texts_to_sequences()**는 생성된 단어 집합에 기반하여 입력으로 주어진 텍스트 데이터의 단어들을 모두 정수로 변환하는 정수 인코딩을 수행해요.\n",
    "현재 단어 집합의 크기를 16,500으로 제한했으니까 이제 16,500이 넘는 숫자들은 정수 인코딩 후에는 데이터에 존재하지 않게 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2214, 66, 18, 3232, 601, 47, 101, 729, 5805, 14659, 2639, 1406, 1372, 178, 62, 1372, 1752, 3998, 2162, 78, 225, 7, 3127, 81, 1499, 80, 3, 91, 43, 56, 220, 236, 1790, 336, 413, 232, 167], [4, 179, 133, 564, 93, 373, 69, 9120, 2688, 1972, 12, 52, 175, 40, 52, 6318, 69, 2719, 6910, 4, 186, 1183, 608, 2824, 219, 1542, 429, 3954, 466, 777, 13, 2553, 1760], [120, 156, 487, 1485, 53, 1, 14660, 120, 1226, 1822, 1823, 811, 5178, 1226, 3870, 124, 29, 2, 434, 333, 515, 241, 3, 3642, 6, 286, 2, 6804, 665, 15580, 1273, 1485, 3]]\n"
     ]
    }
   ],
   "source": [
    "# 텍스트 시퀀스를 정수 시퀀스로 변환\n",
    "encoder_input_train = src_tokenizer.texts_to_sequences(encoder_input_train) \n",
    "encoder_input_test = src_tokenizer.texts_to_sequences(encoder_input_test)\n",
    "\n",
    "#잘 진행되었는지 샘플 출력\n",
    "print(encoder_input_train[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "정수 값으로 잘 변환이 되었네요.\n",
    "\n",
    "이제 **headlines** 데이터에 대해서도 동일한 작업을 수행할게요. 케라스의 토크나이저를 사용하여 **decoder_input_train**을 입력으로 전체 단어 집합과 각 단어에 대한 빈도수를 계산합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "tar_tokenizer = Tokenizer()\n",
    "tar_tokenizer.fit_on_texts(decoder_input_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "headlines 데이터도 이제 단어 집합이 생성되는 동시에 각 단어에 고유한 정수가 부여되어 있고  **tar_tokenizer.word_index**에 저장되어있어요.  \n",
    "\n",
    "**tar_tokenizer.word_counts.items()**에는 단어와 각 단어의 등장 빈도수가 저장되어져 있는데, 이를 통해서 통계적인 정보를 얻은 후 등장 빈도수가 4회 미만인 단어들이 이 데이터에서 얼만큼의 비중을 차지하는지 확인해볼게요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합(vocabulary)의 크기 : 25985\n",
      "등장 빈도가 4번 이하인 희귀 단어의 수: 16564\n",
      "단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기 9421\n",
      "단어 집합에서 희귀 단어의 비율: 63.74446796228593\n",
      "전체 등장 빈도에서 희귀 단어 등장 빈도 비율: 5.53968059183423\n"
     ]
    }
   ],
   "source": [
    "threshold = 5\n",
    "total_cnt = len(tar_tokenizer.word_index) # 단어의 수\n",
    "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
    "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
    "rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
    "\n",
    "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
    "for key, value in tar_tokenizer.word_counts.items():\n",
    "    total_freq = total_freq + value\n",
    "\n",
    "    # 단어의 등장 빈도수가 threshold보다 작으면\n",
    "    if(value < threshold):\n",
    "        rare_cnt = rare_cnt + 1\n",
    "        rare_freq = rare_freq + value\n",
    "\n",
    "print('단어 집합(vocabulary)의 크기 :',total_cnt)\n",
    "print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n",
    "print('단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기 %s'%(total_cnt - rare_cnt))\n",
    "print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n",
    "print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "등장 빈도가 4회 이하인 단어들은 단어 집합에서 약 63%를 차지하고있네요. 하지만, 실제로 훈련 데이터에서 등장 빈도로 차지하는 비중은 상대적으로 적은 수치인 5.53%밖에 되지 않아요. \n",
    "\n",
    "이 단어들도 모두 제거하고 단어 집합의 크기를 9,421에서 어림잡아 9,000으로 단어 집합의 크기를 제한하겠습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input\n",
      "input  [[1, 1061, 963, 3600, 48, 2322, 3, 607, 25, 277, 1374], [1, 10, 514, 269, 6, 73, 3224, 4, 77], [1, 2323, 116, 20, 1437, 3, 206, 3225, 798, 946], [1, 2324, 5226, 3, 1391, 2530, 7, 1415, 360], [1, 1819, 54, 3009, 4, 592, 528, 5, 38, 82]]\n",
      "target\n",
      "decoder  [[1061, 963, 3600, 48, 2322, 3, 607, 25, 277, 1374, 2], [10, 514, 269, 6, 73, 3224, 4, 77, 2], [2323, 116, 20, 1437, 3, 206, 3225, 798, 946, 2], [2324, 5226, 3, 1391, 2530, 7, 1415, 360, 2], [1819, 54, 3009, 4, 592, 528, 5, 38, 82, 2]]\n"
     ]
    }
   ],
   "source": [
    "tar_vocab = 9000\n",
    "tar_tokenizer = Tokenizer(num_words = tar_vocab) \n",
    "tar_tokenizer.fit_on_texts(decoder_input_train)\n",
    "tar_tokenizer.fit_on_texts(decoder_target_train)\n",
    "\n",
    "# 텍스트 시퀀스를 정수 시퀀스로 변환\n",
    "decoder_input_train = tar_tokenizer.texts_to_sequences(decoder_input_train) \n",
    "decoder_target_train = tar_tokenizer.texts_to_sequences(decoder_target_train)\n",
    "decoder_input_test = tar_tokenizer.texts_to_sequences(decoder_input_test)\n",
    "decoder_target_test = tar_tokenizer.texts_to_sequences(decoder_target_test)\n",
    "\n",
    "#잘 변환되었는지 확인\n",
    "print('input')\n",
    "print('input ',decoder_input_train[:5])\n",
    "print('target')\n",
    "print('decoder ',decoder_target_train[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "정상적으로 정수 인코딩 작업이 끝났습니다.\n",
    "\n",
    "빈도수가 낮은 단어를 제거하면서 빈도수가 낮은 단어로만 구성된 샘플들은 단어가 없는 빈(empty) 샘플이 되었을 가능성이 있어요. 이런 현상은 평균길이가 35였던 **text** 데이터보다 평균 길이가 9인 **headlines**에서 더 두드러졌을 가능성이 높습니다.\n",
    "\n",
    "**headlines**에서 길이가 0이 된 샘플들의 인덱스를 받아와볼게요. 여기서 주의할 점은 요약문인 **`decoder_input`**에는 **`sos_token`** 또는 **`decoder_target`**에는 **`eos_token`**이 추가된 상태이고, 이 두 토큰은 모든 샘플에서 등장하므로 빈도수가 샘플수와 동일하게 매우 높으므로 단어 집합 제한에도 삭제 되지 않아요. 그래서 이제 길이가 0이 된 요약문의 실제길이는 1로 나올거에요.   \n",
    "\n",
    "문장의 길이가 0이 된 **`decoder_input`**에는 시작 토큰인 **`sostoken`**, **`decoder_target`**에는 종료 토큰인 **`eostoken`**만 남아 있을테니까요.\n",
    "\n",
    "훈련 데이터와 테스트 데이터에 대해서 요약문의 길이가 1인 경우의 인덱스를 각각 **`drop_train`**과 **`drop_test`**에 라는 변수에 저장하고 삭제하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "삭제할 훈련 데이터의 개수 : 0\n",
      "삭제할 테스트 데이터의 개수 : 0\n",
      "훈련 데이터의 개수 : 52236\n",
      "훈련 레이블의 개수 : 52236\n",
      "테스트 데이터의 개수 : 13059\n",
      "테스트 레이블의 개수 : 13059\n"
     ]
    }
   ],
   "source": [
    "drop_train = [index for index, sentence in enumerate(decoder_input_train) if len(sentence) == 1]\n",
    "drop_test = [index for index, sentence in enumerate(decoder_input_test) if len(sentence) == 1]\n",
    "\n",
    "print('삭제할 훈련 데이터의 개수 :',len(drop_train))\n",
    "print('삭제할 테스트 데이터의 개수 :',len(drop_test))\n",
    "\n",
    "encoder_input_train = np.delete(encoder_input_train, drop_train, axis=0)\n",
    "decoder_input_train = np.delete(decoder_input_train, drop_train, axis=0)\n",
    "decoder_target_train = np.delete(decoder_target_train, drop_train, axis=0)\n",
    "\n",
    "encoder_input_test = np.delete(encoder_input_test, drop_test, axis=0)\n",
    "decoder_input_test = np.delete(decoder_input_test, drop_test, axis=0)\n",
    "decoder_target_test = np.delete(decoder_target_test, drop_test, axis=0)\n",
    "\n",
    "print('훈련 데이터의 개수 :', len(encoder_input_train))\n",
    "print('훈련 레이블의 개수 :',len(decoder_input_train))\n",
    "print('테스트 데이터의 개수 :',len(encoder_input_test))\n",
    "print('테스트 레이블의 개수 :',len(decoder_input_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모든 데이터가 토큰 + 제거되지 않은 단어로 구성이 되어 있네요.  \n",
    "혹시 모르니, 길이가 2 이상인 데이터를 확인해 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "삭제하지 않을 훈련 데이터의 개수 : 52236\n",
      "삭제하지 않을 테스트 데이터의 개수 : 13059\n",
      "훈련 데이터의 개수 : 52236\n",
      "테스트 데이터의 개수 : 13059\n"
     ]
    }
   ],
   "source": [
    "drop_train = [index for index, sentence in enumerate(decoder_input_train) if len(sentence) >= 2]\n",
    "drop_test = [index for index, sentence in enumerate(decoder_input_test) if len(sentence) >= 2]\n",
    "\n",
    "print('삭제하지 않을 훈련 데이터의 개수 :',len(drop_train))\n",
    "print('삭제하지 않을 테스트 데이터의 개수 :',len(drop_test))\n",
    "\n",
    "print('훈련 데이터의 개수 :', len(encoder_input_train))\n",
    "print('테스트 데이터의 개수 :',len(encoder_input_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 이상인 데이터와 총 데이터의 개수가 일치하네요.  \n",
    "여기서 더 추가적인 작업은 하지 않고 다음으로 패딩을 진행하겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 패딩하기\n",
    "\n",
    "텍스트 시퀀스를 정수 시퀀스로 변환했다면, 이제 서로 다른 길이의 샘플들을 병렬 처리하기 위해 같은 길이로 맞춰주는 패딩 작업을 해주어야 해야해요. 아까 정해두었던 최대 길이로 패딩해 줄 거에요. 최대 길이보다 짧은 데이터들은 뒤의 공간에 숫자 0을 넣어 최대 길이로 길이를 맞춰줄게요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_train = pad_sequences(encoder_input_train, maxlen = text_max_len, padding='post')\n",
    "encoder_input_test = pad_sequences(encoder_input_test, maxlen = text_max_len, padding='post')\n",
    "decoder_input_train = pad_sequences(decoder_input_train, maxlen = headlines_max_len, padding='post')\n",
    "decoder_target_train = pad_sequences(decoder_target_train, maxlen = headlines_max_len, padding='post')\n",
    "decoder_input_test = pad_sequences(decoder_input_test, maxlen = headlines_max_len, padding='post')\n",
    "decoder_target_test = pad_sequences(decoder_target_test, maxlen = headlines_max_len, padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 학습에 필요한 데이터 전처리가 모두 끝났습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 설계하기\n",
    "\n",
    "\n",
    "이제는 모델을 설계할 시간이에요. 우선 함수형 API를 이용해서 인코더를 설계해 보겠습니다.\n",
    "\n",
    "인코더 부분은 모델을 직접 설계하고 디코더는 어텐션 매커니즘을 사용하겠습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "\n",
    "# 인코더 설계 시작\n",
    "embedding_dim = 256\n",
    "hidden_size = 256\n",
    "\n",
    "# 인코더\n",
    "encoder_inputs = Input(shape=(text_max_len,))\n",
    "\n",
    "# 인코더의 임베딩 층\n",
    "enc_emb = Embedding(src_vocab, embedding_dim)(encoder_inputs)\n",
    "\n",
    "# 인코더의 LSTM 1\n",
    "encoder_lstm1 = LSTM(hidden_size, return_sequences=True, return_state=True ,dropout = 0.4, recurrent_dropout = 0.4)\n",
    "encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb)\n",
    "\n",
    "# 인코더의 LSTM 2\n",
    "encoder_lstm2 = LSTM(hidden_size, return_sequences=True, return_state=True, dropout=0.4, recurrent_dropout=0.4)\n",
    "encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1)\n",
    "\n",
    "# 인코더의 LSTM 3\n",
    "encoder_lstm3 = LSTM(hidden_size, return_state=True, return_sequences=True, dropout=0.4, recurrent_dropout=0.4)\n",
    "encoder_outputs, state_h, state_c= encoder_lstm3(encoder_output2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "임베딩 벡터의 차원은 128로 정의하고, hidden state의 크기를 256으로 정의했어요.  \n",
    "hidden state는 LSTM에서 얼만큼의 수용력(capacity)를 가질지를 정하는 파라미터에요. 이 파라미터는 LSTM의 용량의 크기나, LSTM에서의 뉴론의 갯수라고 이해하면 됩니다.  \n",
    "- 추가로, 다른 신경망과 마찬가지로, 무조건 용량을 많이 준다고 해서 성능이 반드시 올라가는 것은 아니에요.\n",
    "\n",
    "인코더의 LSTM은 총 3개의 층으로 구성해서 모델의 복잡도를 높였어요. hidden state의 크기를 늘리는 것이 LSTM 층 1개의 용량을 늘린다면, 3개의 층을 사용하는 것은 모델의 용량을 늘린다고 볼 수 있죠.  \n",
    "\n",
    "- 3개의 층을 지나서 인코더로부터 나온 출력 벡터는 디코더로 보내겠습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
     ]
    }
   ],
   "source": [
    "# 디코더 설계\n",
    "\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "\n",
    "# 디코더의 임베딩 층\n",
    "dec_emb_layer = Embedding(tar_vocab, embedding_dim)\n",
    "dec_emb = dec_emb_layer(decoder_inputs)\n",
    "\n",
    "# 디코더의 LSTM\n",
    "decoder_lstm = LSTM(hidden_size, return_sequences = True, return_state = True, dropout = 0.4, recurrent_dropout=0.2)\n",
    "decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state = [state_h, state_c])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "디코더의 임베딩 층과 LSTM을 설계하는 것은 인코더와 거의 동일해요. 하지만 LSTM의 입력을 정의할 때, initial_state의 인자값으로 인코더의 hidden state와 cell state의 값을 넣어줘야 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "디코더의 출력층은 어텐션 매커니즘으로 사용하겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#  어텐션 매커니즘 사용 ( 추상적 요약 )\n",
    "일반적인 seq2seq보다는 어텐션 메커니즘을 적용한 seq2seq를 사용하는 것이 더 나은 성능을 보일 수 있습니다. \n",
    "\n",
    "어텐션 메커니즘을 사용하여 seq2seq를 설계해 봅니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 어텐션 매커니즘\n",
    "어텐션 메커니즘을 수행하는 어텐션 함수를 설계하는 것은 또 다른 새로운 신경망을 설계해야한다는 뜻이에요.  여기서는 이미 구현된 어텐션 함수를 가져와서 디코더의 출력층에 어떤 방식으로 결합하는지 확인하고 어텐션 매커니즘을 사용하겠습니다.\n",
    "\n",
    "깃허브에 공개되어 있는 어텐션 함수를 다운로드 합니다.\n",
    "``` python\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/thushv89/attention_keras/master/src/layers/attention.py\", filename=\"attention.py\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/thushv89/attention_keras/master/src/layers/attention.py\", filename=\"attention.py\")\n",
    "from attention import AttentionLayer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "디코더의 임베딩 층과 LSTM을 설계하는 것은 인코더와 거의 동일합니다. 하지만 LSTM의 입력을 정의할 때, initial_state의 인자값으로 인코더의 hidden state와 cell state의 값을 넣어줘야 해요.\n",
    "\n",
    "디코더의 출력층을 어텐션 함수로 설계해볼게요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 38)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 38, 256)      4224000     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 38, 256), (N 525312      embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 38, 256), (N 525312      lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 256)    2304000     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, 38, 256), (N 525312      lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   [(None, None, 256),  525312      embedding_1[0][0]                \n",
      "                                                                 lstm_2[0][1]                     \n",
      "                                                                 lstm_2[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "attention_layer (AttentionLayer ((None, None, 256),  131328      lstm_2[0][0]                     \n",
      "                                                                 lstm_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concat_layer (Concatenate)      (None, None, 512)    0           lstm_3[0][0]                     \n",
      "                                                                 attention_layer[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 9000)   4617000     concat_layer[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 13,377,576\n",
      "Trainable params: 13,377,576\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 어텐션 층(어텐션 함수)\n",
    "attn_layer = AttentionLayer(name='attention_layer')\n",
    "# 인코더와 디코더의 모든 time step의 hidden state를 어텐션 층에 전달하고 결과를 리턴\n",
    "attn_out, attn_states = attn_layer([encoder_outputs, decoder_outputs])\n",
    "\n",
    "# 어텐션의 결과와 디코더의 hidden state들을 연결\n",
    "decoder_concat_input = Concatenate(axis = -1, name='concat_layer')([decoder_outputs, attn_out])\n",
    "\n",
    "# 디코더의 출력층\n",
    "decoder_softmax_layer = Dense(tar_vocab, activation='softmax')\n",
    "decoder_softmax_outputs = decoder_softmax_layer(decoder_concat_input)\n",
    "\n",
    "# 모델 정의\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_softmax_outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 코드는 인코더의 hidden state들과 디코더의 hidden state들을 어텐션 함수의 입력으로 사용하고, 어텐션 함수가 리턴한 값을 예측 시에 디코더의 hidden state와 함께 활용하는 형태로 작동하고 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 훈련하기\n",
    "\n",
    "설계한 모델을 가지고 훈련을 진행하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "205/205 [==============================] - 64s 311ms/step - loss: 6.2916 - val_loss: 5.9249\n",
      "Epoch 2/50\n",
      "205/205 [==============================] - 62s 300ms/step - loss: 5.7450 - val_loss: 5.5654\n",
      "Epoch 3/50\n",
      "205/205 [==============================] - 61s 300ms/step - loss: 5.3621 - val_loss: 5.2664\n",
      "Epoch 4/50\n",
      "205/205 [==============================] - 61s 298ms/step - loss: 5.0643 - val_loss: 5.1010\n",
      "Epoch 5/50\n",
      "205/205 [==============================] - 61s 298ms/step - loss: 4.8274 - val_loss: 4.8988\n",
      "Epoch 6/50\n",
      "205/205 [==============================] - 61s 299ms/step - loss: 4.6238 - val_loss: 4.7814\n",
      "Epoch 7/50\n",
      "205/205 [==============================] - 61s 299ms/step - loss: 4.4422 - val_loss: 4.6365\n",
      "Epoch 8/50\n",
      "205/205 [==============================] - 61s 299ms/step - loss: 4.2798 - val_loss: 4.5749\n",
      "Epoch 9/50\n",
      "205/205 [==============================] - 61s 300ms/step - loss: 4.1302 - val_loss: 4.4769\n",
      "Epoch 10/50\n",
      "205/205 [==============================] - 61s 297ms/step - loss: 3.9948 - val_loss: 4.4757\n",
      "Epoch 11/50\n",
      "205/205 [==============================] - 61s 300ms/step - loss: 3.8719 - val_loss: 4.3712\n",
      "Epoch 12/50\n",
      "205/205 [==============================] - 62s 301ms/step - loss: 3.7579 - val_loss: 4.3558\n",
      "Epoch 13/50\n",
      "205/205 [==============================] - 62s 300ms/step - loss: 3.6558 - val_loss: 4.3373\n",
      "Epoch 14/50\n",
      "205/205 [==============================] - 61s 297ms/step - loss: 3.5592 - val_loss: 4.2665\n",
      "Epoch 15/50\n",
      "205/205 [==============================] - 61s 296ms/step - loss: 3.4684 - val_loss: 4.2514\n",
      "Epoch 16/50\n",
      "205/205 [==============================] - 61s 296ms/step - loss: 3.3841 - val_loss: 4.2280\n",
      "Epoch 17/50\n",
      "205/205 [==============================] - 61s 297ms/step - loss: 3.3023 - val_loss: 4.2264\n",
      "Epoch 18/50\n",
      "205/205 [==============================] - 61s 298ms/step - loss: 3.2276 - val_loss: 4.2078\n",
      "Epoch 19/50\n",
      "205/205 [==============================] - 61s 296ms/step - loss: 3.1540 - val_loss: 4.2007\n",
      "Epoch 20/50\n",
      "205/205 [==============================] - 61s 296ms/step - loss: 3.0878 - val_loss: 4.1995\n",
      "Epoch 21/50\n",
      "205/205 [==============================] - 61s 296ms/step - loss: 3.0239 - val_loss: 4.1911\n",
      "Epoch 22/50\n",
      "205/205 [==============================] - 61s 296ms/step - loss: 2.9661 - val_loss: 4.1851\n",
      "Epoch 23/50\n",
      "205/205 [==============================] - 61s 296ms/step - loss: 2.9074 - val_loss: 4.2029\n",
      "Epoch 24/50\n",
      "205/205 [==============================] - 61s 295ms/step - loss: 2.8496 - val_loss: 4.1887\n",
      "Epoch 00024: early stopping\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience = 2)\n",
    "history = model.fit(x = [encoder_input_train, decoder_input_train], y = decoder_target_train, \\\n",
    "          validation_data = ([encoder_input_test, decoder_input_test], decoder_target_test),\n",
    "          batch_size = 256, callbacks=[es], epochs = 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EarlyStopping을 사용해서 **val_loss**가 증가하는 **`patiensce =2`** 2회 관측되면 학습을 조기에 멈추도록 설정했습니다.\n",
    "\n",
    "20번째 epoch쯤에서 조기종료되네요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 인퍼런스 모델 구현하기\n",
    "\n",
    "테스트 단계에서는 정수 인덱스 행렬로 존재하던 텍스트 데이터를 실제 데이터로 복원해야 하므로, 필요한 3개의 사전을 아래와 같이 미리 준비해 둡니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_index_to_word = src_tokenizer.index_word # 원문 단어 집합에서 정수 -> 단어를 얻음\n",
    "tar_word_to_index = tar_tokenizer.word_index # 요약 단어 집합에서 단어 -> 정수를 얻음\n",
    "tar_index_to_word = tar_tokenizer.index_word # 요약 단어 집합에서 정수 -> 단어를 얻음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "seq2seq는 훈련할 때와 실제 동작할 때(인퍼런스 단계)의 방식이 다르므로 그에 맞게 모델 설계를 별개로 진행해야 한다는 것, 알고 계시나요?\n",
    "\n",
    "훈련 단계에서는 디코더의 입력부에 정답이 되는 문장 전체를 한꺼번에 넣고 디코더의 출력과 한번에 비교할 수 있으므로, 인코더와 디코더를 엮은 통짜 모델 하나만 준비했습니다.\n",
    "\n",
    "그러나 정답 문장이 없는 인퍼런스 단계에서는 만들어야 할 문장의 길이만큼 디코더가 반복 구조로 동작해야 하기 때문에 부득이하게 인퍼런스를 위한 모델 설계를 별도로 해주어야 합니다. 이때는 인코더 모델과 디코더 모델을 분리해서 설계합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인코더 설계\n",
    "encoder_model = Model(inputs=encoder_inputs, outputs=[encoder_outputs, state_h, state_c])\n",
    "\n",
    "# 이전 시점의 상태들을 저장하는 텐서\n",
    "decoder_state_input_h = Input(shape=(hidden_size,))\n",
    "decoder_state_input_c = Input(shape=(hidden_size,))\n",
    "\n",
    "dec_emb2 = dec_emb_layer(decoder_inputs)\n",
    "# 문장의 다음 단어를 예측하기 위해서 초기 상태(initial_state)를 이전 시점의 상태로 사용. 이는 뒤의 함수 decode_sequence()에 구현\n",
    "# 훈련 과정에서와 달리 LSTM의 리턴하는 은닉 상태와 셀 상태인 state_h와 state_c를 버리지 않음.\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "어텐션 메커니즘을 사용하는 출력층을 설계합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 어텐션 함수\n",
    "decoder_hidden_state_input = Input(shape=(text_max_len, hidden_size))\n",
    "attn_out_inf, attn_states_inf = attn_layer([decoder_hidden_state_input, decoder_outputs2])\n",
    "decoder_inf_concat = Concatenate(axis=-1, name='concat')([decoder_outputs2, attn_out_inf])\n",
    "\n",
    "# 디코더의 출력층\n",
    "decoder_outputs2 = decoder_softmax_layer(decoder_inf_concat) \n",
    "\n",
    "# 최종 디코더 모델\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + [decoder_hidden_state_input,decoder_state_input_h, decoder_state_input_c],\n",
    "    [decoder_outputs2] + [state_h2, state_c2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "인퍼런스 단계에서 단어 시퀀스를 완성하는 함수입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # 입력으로부터 인코더의 상태를 얻음\n",
    "    e_out, e_h, e_c = encoder_model.predict(input_seq)\n",
    "\n",
    "     # <SOS>에 해당하는 토큰 생성\n",
    "    target_seq = np.zeros((1,1))\n",
    "    target_seq[0, 0] = tar_word_to_index['sostoken']\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition: # stop_condition이 True가 될 때까지 루프 반복\n",
    "\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + [e_out, e_h, e_c])\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_token = tar_index_to_word[sampled_token_index]\n",
    "\n",
    "        if(sampled_token!='eostoken'):\n",
    "            decoded_sentence += ' '+sampled_token\n",
    "\n",
    "        #  <eos>에 도달하거나 최대 길이를 넘으면 중단.\n",
    "        if (sampled_token == 'eostoken'  or len(decoded_sentence.split()) >= (headlines_max_len-1)):\n",
    "            stop_condition = True\n",
    "\n",
    "        # 길이가 1인 타겟 시퀀스를 업데이트\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # 상태를 업데이트 합니다.\n",
    "        e_h, e_c = h, c\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 실제 결과와 요약문 비교하기 (추상적 요약)\n",
    "원래의 요약문(headlines 열)과 학습을 통해 얻은 추상적 요약의 결과를 비교해봅니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##  모델 테스트하기\n",
    "\n",
    "테스트 단계에서는 정수 시퀀스를 텍스트 시퀀스로 변환하여 결과를 확인하는 것이 편하겠죠. 주어진 정수 시퀀스를 텍스트 시퀀스로 변환하는 함수를 만들어볼게요. 함수를 만들 때, Text의 정수 시퀀스에서는 패딩을 위해 사용되는 숫자 0을 제외시키고 Summary의 정수 시퀀스에서는 숫자 0, 시작 토큰의 인덱스, 종료 토큰의 인덱스를 출력에서 제외시키도록 만들거에요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원문의 정수 시퀀스를 텍스트 시퀀스로 변환\n",
    "def seq2text(input_seq):\n",
    "    temp=''\n",
    "    for i in input_seq:\n",
    "        if(i!=0):\n",
    "            temp = temp + src_index_to_word[i]+' '\n",
    "    return temp\n",
    "\n",
    "# 요약문의 정수 시퀀스를 텍스트 시퀀스로 변환\n",
    "def seq2summary(input_seq):\n",
    "    temp=''\n",
    "    for i in input_seq:\n",
    "        if((i!=0 and i!=tar_word_to_index['sostoken']) and i!=tar_word_to_index['eostoken']):\n",
    "            temp = temp + tar_index_to_word[i] + ' '\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "테스트 데이터 약 50개의 샘플에 대해서 실제 요약과 예측된 요약을 비교해보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원문 : actor siddharth tweet addressed bjp wrote dear bjp power empower india stay people private choices stop hindu nation narrative better added row cattle markets unnecessary people centre stay allowing slaughter \n",
      "실제 요약 : bjp stop this hindu nation siddharth \n",
      "예측 요약 :  bjp should not be in india bjp leader\n",
      "\n",
      "\n",
      "원문 : cnn news anchor karma ended last bulletin channel friday hoping clear popular wear pants stood audience said us wear trousers see right ended karma signing trousers \n",
      "실제 요약 : cnn anchor announces resignation says do wear pants \n",
      "예측 요약 :  uk man once turned into white house\n",
      "\n",
      "\n",
      "원문 : market capitalisation cryptocurrencies would hit trillion mark said powell founder ceo world th largest cryptocurrency exchange volume said cryptocurrency market would continue see growth despite recent price plunge combined market value cryptocurrencies currently billion \n",
      "실제 요약 : crypto market will be worth tn this year ceo \n",
      "예측 요약 :  bitcoin hits trillion in market capitalisation\n",
      "\n",
      "\n",
      "원문 : inaugural address pakistan prime minister imran khan said time change country destiny adding spoken neighbouring countries improve bilateral relations need peace without cannot improve pakistan situation khan added slammed previous government current debt crisis \n",
      "실제 요약 : it is time to change pakistan pm imran khan \n",
      "예측 요약 :  india is the country of the country pak pm\n",
      "\n",
      "\n",
      "원문 : government kazakhstan appointed chairman jewellers suraj honorary consul southern states tamil nadu karnataka kerala kazakhstan ambassador india said confident honorary consul office would promote development mutually beneficial cooperation two countries \n",
      "실제 요약 : indian businessman appointed kazakhstan honorary \n",
      "예측 요약 :  who is the most expensive in india\n",
      "\n",
      "\n",
      "원문 : stop illegal burning open delhi government started deployment environment marshals environment minister imran hussain said instructed act eyes ears environment department report violations wards added violations include burning leaves plastic open dust pollution due construction among others \n",
      "실제 요약 : delhi appoints environment to curb burning in open \n",
      "예측 요약 :  govt plans to ensure illegal slaughterhouses in india\n",
      "\n",
      "\n",
      "원문 : roger federer match robin open hit ace third set called federer going take second serve told officials called federer insisted umpire called fault federer took second serve \n",
      "실제 요약 : roger federer calls his own serve out after hitting \n",
      "예측 요약 :  roger federer gets maiden maiden wimbledon final\n",
      "\n",
      "\n",
      "원문 : punjab state commission protection child rights claimed rescued total juvenile beggars last two months many rescued children admitted schools commission chairman said officials added poor migrants mainly belonging northeastern states pushed children begging \n",
      "실제 요약 : punjab child rights body saves minor beggars in months \n",
      "예측 요약 :  child child children in khand govt\n",
      "\n",
      "\n",
      "원문 : chhattisgarh chief minister raman singh led bjp government defeated confidence motion congress moved state assembly debate started friday ran hours ended bjp government winning motion saturday several issues including sex cd case raised \n",
      "실제 요약 : chhattisgarh cm defeats no confidence motion against him \n",
      "예측 요약 :  bjp leader takes oath as congress\n",
      "\n",
      "\n",
      "원문 : delhi cm arvind kejriwal thursday questioned behind centre note ban move termed self inflicted deep wound indian economy alleged list financial scams pm narendra modi government second anniversary demonetisation kejriwal questioned country pushed disaster \n",
      "실제 요약 : reason behind demonetisation still mystery kejriwal \n",
      "예측 요약 :  delhi cm slams delhi for ban on demonetisation\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(30, 40):\n",
    "    print(\"원문 :\", seq2text(encoder_input_test[i]))\n",
    "    print(\"실제 요약 :\", seq2summary(decoder_input_test[i]))\n",
    "    print(\"예측 요약 :\", decode_sequence(encoder_input_test[i].reshape(1, text_max_len)))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Summa을 이용해서 추출적 요약해보기\n",
    "추상적 요약은 추출적 요약과는 달리 문장의 표현력을 다양하게 가져갈 수 있지만, 추출적 요약에 비해서 난이도가 높아요. 반대로 말하면 추출적 요약은 추상적 요약에 비해 난이도가 낮고 기존 문장에서 문장을 꺼내오는 것이므로 잘못된 요약이 나올 가능성이 낮아요.\n",
    "\n",
    "Summa의 summarize를 사용하여 추출적 요약을 해봅니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### summa 데이터 다운"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from summa.summarizer import summarize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "summar의 summarize 매트릭스 시놉시스를 다운로드 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = requests.get('http://rare-technologies.com/the_matrix_synopsis.txt').text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "text에는 매트릭스 시놉시스가 문자열로 저장되어져 있어요. 출력 결과가 아주 길기 때문에 일부만 출력해보고, 잘 저장이 되었는지 확인해봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The screen is filled with green, cascading code which gives way to the title, The Matrix.\r\n",
      "\r\n",
      "A phone rings and text appears on the screen: \"Call trans opt: received. 2-19-98 13:24:18 REC: Log>\" As a conversation takes place between Trinity (Carrie-Anne Moss) and Cypher (Joe Pantoliano), two free humans, a table of random green numbers are being scanned and individual numbers selected, creating a series of digits not unlike an ordinary phone number, as if a code is being deciphered or a call is being traced.\r\n",
      "\r\n",
      "Trinity discusses some unknown person. Cypher taunts Trinity, suggesting she enjoys watching him. Trinity counters that \"Morpheus (Laurence Fishburne) says he may be 'the One',\" just as the sound of a number being selected alerts Trinity that someone may be tracing their call. She ends the call.\r\n",
      "\r\n",
      "Armed policemen move down a darkened, decrepit hallway in the Heart O' the City Hotel, their flashlight beam bouncing just ahead of them. They come to room 303, kick down the door and \n"
     ]
    }
   ],
   "source": [
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summa의 summarize는 문장 토큰화를 별도로 하지 않더라도 내부적으로 문장 토큰화를 수행합니다.  \n",
    "그렇기 때문에 문장 구분이 되어있지 않은 원문을 바로 입력으로 넣을 수 있어요.  \n",
    "원문의 0.01%만을 출력해도록 설정비율을 주어서 요약문의 문장 개수를 줄여보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trinity takes Neo to Morpheus.\n",
      "Morpheus, Trinity, Neo, Apoc, Switch, Mouse and Cypher are jacked into the Matrix.\n",
      "Morpheus, who is above Neo in the walls, breaks through the wall and lands on the agent, yelling to Trinity to get Neo out of the building.\n",
      "Trinity brings the helicopter down to the floor that Morpheus is on and Neo opens fire on the three Agents.\n"
     ]
    }
   ],
   "source": [
    "print(summarize(text, ratio=0.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "단어의 수로 요약문의 크기를 조절할 수 있습니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "summary :\n",
      "Trinity takes Neo to Morpheus.\n",
      "Morpheus, Trinity, Neo, Apoc, Switch, Mouse and Cypher are jacked into the Matrix.\n",
      "Morpheus, who is above Neo in the walls, breaks through the wall and lands on the agent, yelling to Trinity to get Neo out of the building.\n",
      "Trinity brings the helicopter down to the floor that Morpheus is on and Neo opens fire on the three Agents.\n"
     ]
    }
   ],
   "source": [
    "print('summary :')\n",
    "print(summarize(text, words=60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "문장의 비율의 0.01과 words 60의 요약 문장 내용이 동일하네요.   \n",
    "이러면 words 60개가 0.01의 비율인 것을 알 수 있게 되었네요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Abstractive** 같은 경우 본문의 내용에 없는 단어도 사용이 가능한것을 확인할 수 있습니다.  \n",
    "그렇지만, 추상적인 내용도 있어서 원문의 직접적인 단어가 들어간 **Extractive Summarization**이 좀 더 핵심을 짚는 능력은 더 좋은것 같습니다.\n",
    "\n",
    "하지만, **Extractive Summarization**로 할 경우 문장의 길이가 길어야 요약이 제대로 되지만, **Abstractive**로 할 경우 **Extractive**보다 상대적으로 짧아도 문장의 요약이 가능합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 비율이 0.001로 너무 짧아서 요약이 안됩니다.\n",
    "print(summarize(text, ratio=0.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35932\n",
      "The screen is filled with green, cascading code which gives way to the title, The Matrix.\r\n",
      "\r\n",
      "A phone rings and text appears on the screen: \"Call trans opt: received. 2-19-98 13:24:18 REC: Log>\" As a conversation takes place between Trinity (Carrie-Anne Moss) and Cypher (Joe Pantoliano), two free humans, a table of random green numbers are being scanned and individual numbers selected, creating a series of digits not unlike an ordinary phone number, as if a code is being deciphered or a call is being traced.\r\n",
      "\r\n",
      "Trinity discusses some unknown person. Cypher taunts Trinity, suggesting she enjoys watching him. Trinity counters that \"Morpheus (Laurence Fishburne) says he may be 'the One',\" just as the sound of a number being selected alerts Trinity that someone may be tracing their call. She ends the call.\r\n",
      "\r\n",
      "Armed policemen move down a darkened, decrepit hallway in the Heart O' the City Hotel, their flashlight beam bouncing just ahead of them. They come to room 303, kick down the door and \n"
     ]
    }
   ],
   "source": [
    "# 문장을 요약하는데 길이가 많이 필요하다.\n",
    "# 추출적 요약(Extractive Summarization)\n",
    "print(len(text))\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trinity brings the helicopter down to the floor that Morpheus is on and Neo opens fire on the three Agents.\n"
     ]
    }
   ],
   "source": [
    "# 긴 문장을 짧은 단어로 추출적 요약하려 하니 내용이 너무 짧게 요약이 되었다.\n",
    "print(summarize(text, words=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "추상적 요약(Abstractive Summarization)은 더 짧은 길이에도 요약을 할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래에 첫번째 문장의 경우 필요없는 단어도 있지만 요약된 내용에서 serna가 사진을 공유한다는 내용이 들어가 있어서 괜찮게 요약이 되었음을 알 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원문 : reacting tennis player serena williams confirming pregnancy former world number one andy tweeted gonna baby goat referred greatest time often used serena player weeks pregnant confirmed pregnancy wednesday posting picture baby bump snapchat \n",
      "실제 요약 : be baby goat ex world no on serena pregnancy \n",
      "예측 요약 :  serena williams shares picture with serena\n"
     ]
    }
   ],
   "source": [
    "print(\"원문 :\", seq2text(encoder_input_test[11]))\n",
    "print(\"실제 요약 :\", seq2summary(decoder_input_test[11]))\n",
    "print(\"예측 요약 :\", decode_sequence(encoder_input_test[11].reshape(1, text_max_len)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 아래 문장도 요약이 괜찮게 이루어 졌습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원문 : market capitalisation cryptocurrencies would hit trillion mark said powell founder ceo world th largest cryptocurrency exchange volume said cryptocurrency market would continue see growth despite recent price plunge combined market value cryptocurrencies currently billion \n",
      "예측 요약 :  bitcoin hits trillion in market capitalisation\n"
     ]
    }
   ],
   "source": [
    "print(\"원문 :\", seq2text(encoder_input_test[32]))\n",
    "print(\"예측 요약 :\", decode_sequence(encoder_input_test[32].reshape(1, text_max_len)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**추출적 요약**은 보면 주어진 문장에서 뽑아서 요약을 하므로 문장을 해석했을때, 어색하게 문장이 이어지지만,  **추상적 요약**은 문장이 좀 더 부드럽게 이어집니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 추출적 요약(Extractive Summarization)\n",
    "Trinity brings the helicopter down to the floor that Morpheus is on and Neo opens fire on the three Agents.\n",
    "\n",
    "- Trinity는 헬리콥터를 Morpheus가있는 바닥으로 내려 가고 Neo는 세 명의 요원에게 발포합니다.\n",
    "\n",
    "\n",
    "#### 추상적 요약(Abstractive Summarization)\n",
    "bitcoin hits trillion in market capitalisation\n",
    "- 비트 코인, 시가 총액 수조에 도달\n",
    "\n",
    "\n",
    "추출적 요약보다 추상적 요약이 글로만 보았을때는 더 잘 읽히네요.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 회고\n",
    "\n",
    "- 문장 요약이 네이버의 기사와 같은 요약처럼은 깔끔하고 핵심적인 요약을 쉽게 할 수 있을줄 알았지만, 하다보니 원하는 부분의 요약이 안되는 경우도 있어서 기존의 네이버 기사 3줄 요약이 간단해 보이지만 어렵다는것을 알 게 되었던것 같습니다.\n",
    "\n",
    "\n",
    "- 하면서 추출적 요약이 확실히 추상적 요약보단 난이도가 쉬워서 좋은것 같지만, 문장의 길이나 input을 받은 문장에서만 요약을할 수 있어서 제한적이라는 생각이 들었습니다.  \n",
    "\n",
    "\n",
    "- 그리고 프로젝트를 하면서 **추출적 요약**은 내용의 어떤 **편향**적인 요소가 없는 문장 자체의 내용에서 핵심 부분을 보일때 사용하는것이 좋은것 같고 **추상적 요약**은 괜찮은 모델을 만든다면, 사람의 생각이 어느정도 반영된 요약을 흉내낼 수 있을것 같다는 생각이 들었습니다.    \n",
    "<br>같은 기사가 주어졌을때 IT 부분을 위주로 학습시켜서 어떤 기사의 IT부분을 더 강조해서 요약한다거나 아니면 경제 부분을 더 요약한다거나 하는 등의 방식으로도 할 수 있을것 같아서 여러 생각을 가지고 프로젝트를 할 수 있어서 좋았습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
